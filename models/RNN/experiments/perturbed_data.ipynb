{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Explanations generated by Influence Function, RPS-$l_2$, and RPS-LJE for perturbed samples with LSTM on sentiment analysis task\n",
    "Table 3 (Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "import random\n",
    "from torchtext import datasets\n",
    "from models.RNN.RNN import initialize_RNN\n",
    "from models.RNN.utils_imdb import create_fields, build_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import config\n",
    "TEXT, LABEL = create_fields()\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root='{}/.data'.format(config.project_root))\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(1234))\n",
    "print('------------------building vocab---------------------')\n",
    "# build vocab\n",
    "build_vocab(train_data, TEXT, LABEL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = initialize_RNN(TEXT, use_pretrained=True).to(device)\n",
    "model.load_state_dict(torch.load('../saved_models/base/model/sentiment-model.pt'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    intermediate_x, prediction = model(tensor, length_tensor, return_hidden=True)\n",
    "    prediction = torch.sigmoid(prediction)\n",
    "    return intermediate_x.squeeze().cpu().detach().numpy(), prediction.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = '../saved_models/base'\n",
    "file = np.load('{}/model/saved_outputs.npz'.format(path))\n",
    "intermediate_train = torch.from_numpy(file['intermediate_train']).squeeze()\n",
    "labels_train = file['labels_train']\n",
    "pred_train = file['pred_train'].squeeze()\n",
    "\n",
    "wrongly_predicted_train_ids = np.argwhere(np.abs(np.round(pred_train)-labels_train)>0).flatten()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "self_influence = np.load('{}/calculated_weights/ours_weight_matrix_with_lr_1e-05.npz'.format(path),\n",
    "                         allow_pickle=True)['self_influence'].squeeze()\n",
    "order = [i for i in np.flip(np.argsort(np.abs(self_influence))) if not i in wrongly_predicted_train_ids]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weight_matrix_rep = np.load('{}/calculated_weights/representer_weight_matrix.npz'.format(path), allow_pickle=True)['weight_matrix']\n",
    "weight_matrix_influence = np.load('{}/calculated_weights/influence_weight_matrix.npz'.format(path), allow_pickle=True)['weight_matrix'].squeeze()\n",
    "weight_matrix_ours = np.load('{}/calculated_weights/ours_weight_matrix_with_lr_1e-05.npz'.format(path), allow_pickle=True)['weight_matrix'].squeeze()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_representer_order(intermediate_test, true_class=0):\n",
    "    tmp = weight_matrix_rep[:,0] * np.dot(intermediate_train,\n",
    "                                           intermediate_test)\n",
    "    if true_class == 1:\n",
    "        pos_idx = np.flip(np.argsort(tmp), axis=0)\n",
    "    else:\n",
    "        pos_idx = np.argsort(tmp)\n",
    "    return pos_idx\n",
    "\n",
    "def get_influence_order(intermediate_test, pred_test, true_class=0):\n",
    "    jaccobian_test = (pred_test - true_class)*intermediate_test\n",
    "    tmp = jaccobian_test@ np.transpose(weight_matrix_influence)\n",
    "    pos_idx = np.argsort(tmp, axis=0)\n",
    "    return pos_idx\n",
    "\n",
    "def get_ours_order(intermediate_test, true_class=0):\n",
    "    tmp = np.dot(weight_matrix_ours, intermediate_test)\n",
    "    if true_class == 1:\n",
    "        pos_idx = np.flip(np.argsort(tmp), axis=0)\n",
    "    else:\n",
    "        pos_idx = np.argsort(tmp)\n",
    "    return pos_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_text(data):\n",
    "    return ' '.join([str(elem) for elem in data.text])\n",
    "def get_label(data):\n",
    "    sentiment = {'neg':0,'pos':1}\n",
    "    return sentiment[data.label]\n",
    "    # return data.label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perturb a training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "\n",
    "perturbed_samples = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow , this was another good spin off of the original American pie ,\n",
      "not as good as band camp , but definitely a lot better the naked mile\n",
      ". Dwight and Erik stifler lead the comedy in this one , but I actually\n",
      "preferred the dialogue in this one to the naked mile . The script was\n",
      "written a lot better \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "id = 2619\n",
    "true_class = get_label(train_data[id])\n",
    "print(textwrap.fill(get_text(train_data[id]))[:300])\n",
    "print(get_label(train_data[id]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "sentence = get_text(train_data[id])\n",
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "tokenized[5] = 'great'\n",
    "perturbed = ' '.join([str(elem) for elem in tokenized])\n",
    "perturbed_samples[id] = perturbed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply the best Estonian film that I have ever seen , although it is\n",
      "made by a Finnish director Ilkka JÃ¤rvi - Laturi . Tallin Pimeduses is\n",
      "an entertaining thriller about a bunch of gangsters who are trying to\n",
      "steal a huge amount of gold , a national treasure that belongs to the\n",
      "republic of Estonia .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "id = 4789\n",
    "true_class = get_label(train_data[id])\n",
    "print(textwrap.fill(get_text(train_data[id]))[:300])\n",
    "print(get_label(train_data[id]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "sentence = get_text(train_data[id])\n",
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "tokenized[2] = 'greatest'\n",
    "perturbed = ' '.join([str(elem) for elem in tokenized])\n",
    "perturbed_samples[id] = perturbed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ca n't tell you all how much I love this movie . I have read reviews\n",
      "that say that this move is \" too confusing \" or \" like swimming in\n",
      "drying concrete \" . I say that these reviewers have no imagination !\n",
      "For anyone who loves Fantasy Fiction , this movie is for you . If you\n",
      "ever loved playing Dung\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "id = 11177\n",
    "true_class = get_label(train_data[id])\n",
    "print(textwrap.fill(get_text(train_data[id]))[:300])\n",
    "print(get_label(train_data[id]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "sentence = get_text(train_data[id])\n",
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "tokenized[9] = 'like'\n",
    "perturbed = ' '.join([str(elem) for elem in tokenized])\n",
    "perturbed_samples[id] = perturbed\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "find orders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def experiment_with_perturbed_data(train_pt, perturbed):\n",
    "    columns=['method','id','sentiment class','review text']\n",
    "    data_array = []\n",
    "\n",
    "    intermediate_x, prediction = predict_sentiment(model, perturbed)\n",
    "    true_class = get_label(train_data[train_pt])\n",
    "    data_array.append(['Original training sample',train_pt,\n",
    "                            get_label(train_data[train_pt]),\n",
    "                            get_text(train_data[train_pt])])\n",
    "\n",
    "    data_array.append(['Perturbed training sample', train_pt,\n",
    "                            get_label(train_data[train_pt]),\n",
    "                            perturbed])\n",
    "\n",
    "    order_ours = get_ours_order(intermediate_x, true_class=true_class)\n",
    "    ours_idx_pos = [i for i in order_ours if not i in wrongly_predicted_train_ids]\n",
    "\n",
    "    order_rep = get_representer_order(intermediate_x, true_class=true_class)\n",
    "    rep_idx_pos = [i for i in order_rep if not i in wrongly_predicted_train_ids]\n",
    "\n",
    "    order_inf = get_influence_order(intermediate_x, prediction, true_class=true_class)\n",
    "    inf_idx_pos = [i for i in order_inf if not i in wrongly_predicted_train_ids]\n",
    "\n",
    "    inf_data = train_data[inf_idx_pos[0]]\n",
    "    data_array.append(['Influence function', inf_idx_pos[0], get_label(inf_data), get_text(inf_data)])\n",
    "\n",
    "    rep_data = train_data[rep_idx_pos[0]]\n",
    "    data_array.append(['RPS-$l_2$', rep_idx_pos[0],get_label(rep_data), get_text(rep_data)])\n",
    "\n",
    "    ours_data = train_data[ours_idx_pos[0]]\n",
    "    data_array.append(['RPS-LJE', ours_idx_pos[0], get_label(ours_data), get_text(ours_data)])\n",
    "\n",
    "    df = pd.DataFrame(data=data_array, columns=columns)\n",
    "    df.to_csv('results/perturbed_train_pt_{}_pos.csv'.format(train_pt))\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "for train_pt, perturbed in perturbed_samples.items():\n",
    "    experiment_with_perturbed_data(train_pt, perturbed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6ec1b404",
   "language": "python",
   "display_name": "PyCharm (MAPSED)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}